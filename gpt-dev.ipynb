{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Building the Base Model\n",
    "\n",
    "We will build a base Bigram model to make simple token prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Dataset to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets get the tiny shakespeare dataset - uncomment to download again!\n",
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in chars:  1115394\n"
     ]
    }
   ],
   "source": [
    "# check the length of the text\n",
    "print (\"length of dataset in chars: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets check the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Find the unique characters that occur in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocabulary_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer\n",
    "\n",
    "Create a Simple Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a very simple tokenizer that maps characters to integers and vice versa\n",
    "# Look at the other tokenizers that are used.\n",
    "# Google uses SentencePiece.  OpenAI uses tiktoken.\n",
    "str_to_int = {ch:i for i, ch in enumerate(chars)}\n",
    "int_to_str = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [str_to_int[ch] for ch in s]\n",
    "decode = lambda x: ''.join([int_to_str[i] for i in x]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 53, 53, 53, 1, 58, 46, 43, 56, 43]\n",
      "helloooo there\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"helloooo there\"))\n",
    "print(decode(encode(\"helloooo there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the Dataset\n",
    "\n",
    "Use the tokenizer to encode the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a tokenizer, let's encode the entire dataset \n",
    "# first we need to store in a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])  # this is the same first 1000 characters we printed earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data set into training and validation splits\n",
    "# We will use 90% of the data for training and 10% for validation (change n for different percentages)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "validation_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set maximum length (block size)\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is tensor([18]) the target is 47\n",
      "When the input is tensor([18, 47]) the target is 56\n",
      "When the input is tensor([18, 47, 56]) the target is 57\n",
      "When the input is tensor([18, 47, 56, 57]) the target is 58\n",
      "When the input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "# Inputs to the transformer\n",
    "x = train_data[:block_size]\n",
    "\n",
    "# the target\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]  # all the inputs up to time t\n",
    "    target = y[t] # the target at time t\n",
    "    print(f\"When the input is {context} the target is {target}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----------\n",
      "When the input is tensor([24]) the target is 43\n",
      "When the input is tensor([24, 43]) the target is 58\n",
      "When the input is tensor([24, 43, 58]) the target is 5\n",
      "When the input is tensor([24, 43, 58,  5]) the target is 57\n",
      "When the input is tensor([24, 43, 58,  5, 57]) the target is 1\n",
      "When the input is tensor([24, 43, 58,  5, 57,  1]) the target is 46\n",
      "When the input is tensor([24, 43, 58,  5, 57,  1, 46]) the target is 43\n",
      "When the input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is 39\n",
      "When the input is tensor([44]) the target is 53\n",
      "When the input is tensor([44, 53]) the target is 56\n",
      "When the input is tensor([44, 53, 56]) the target is 1\n",
      "When the input is tensor([44, 53, 56,  1]) the target is 58\n",
      "When the input is tensor([44, 53, 56,  1, 58]) the target is 46\n",
      "When the input is tensor([44, 53, 56,  1, 58, 46]) the target is 39\n",
      "When the input is tensor([44, 53, 56,  1, 58, 46, 39]) the target is 58\n",
      "When the input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is 1\n",
      "When the input is tensor([52]) the target is 58\n",
      "When the input is tensor([52, 58]) the target is 1\n",
      "When the input is tensor([52, 58,  1]) the target is 58\n",
      "When the input is tensor([52, 58,  1, 58]) the target is 46\n",
      "When the input is tensor([52, 58,  1, 58, 46]) the target is 39\n",
      "When the input is tensor([52, 58,  1, 58, 46, 39]) the target is 58\n",
      "When the input is tensor([52, 58,  1, 58, 46, 39, 58]) the target is 1\n",
      "When the input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is 46\n",
      "When the input is tensor([25]) the target is 17\n",
      "When the input is tensor([25, 17]) the target is 27\n",
      "When the input is tensor([25, 17, 27]) the target is 10\n",
      "When the input is tensor([25, 17, 27, 10]) the target is 0\n",
      "When the input is tensor([25, 17, 27, 10,  0]) the target is 21\n",
      "When the input is tensor([25, 17, 27, 10,  0, 21]) the target is 1\n",
      "When the input is tensor([25, 17, 27, 10,  0, 21,  1]) the target is 54\n",
      "When the input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # How many independent sequences to train on in parallel\n",
    "block_size = 8 # The maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "     data = train_data if split == 'train' else validation_data\n",
    "\n",
    "    # pick a random starting point in the data (batch_size number of random offsets) \n",
    "     ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # batch inputs (x) and targets (y)\n",
    "     x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "     y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "     \n",
    "     return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('----------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "     for t in range(block_size):\n",
    "         context = xb[b, :t+1]\n",
    "         target = yb[b, t]\n",
    "         print(f\"When the input is {context} the target is {target}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Language Model\n",
    "Simplest possible neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targers are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape logits so we can use cross-entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)  # -1 means \"infer this dimension\" (translates to B*T)\n",
    "\n",
    "            # evaluate the loss function (quality of predictions)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "    # idx is a (B, T) array of indices in the current \n",
    "    # the job of generate is to take a BxT and return a BxT+1, +2, +3, etc\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1)    # (B, C)\n",
    "\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "        \n",
    "    \n",
    "m = BigramLanguageModel(vocabulary_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# if we calculate loss we're expecting -ln(1/65) which is close to what our value is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long) # creating a 1x1 tensor of zeros (remember 0 = new line)\n",
    "\n",
    "# ask for 100 new tokens, generate, convert to list to feed into decode\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "#context = torch.zeros((1, 1), dtype=torch.long)\n",
    "#output = m.generate(context, max_new_tokens=100)\n",
    "#print(output)\n",
    "#print(decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output is garbage because the model is untrained.  Lets run the next few steps and do this again after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # I think this is .001\n",
    "# basically take gradients and update based on loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a typical training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.692410945892334\n",
      "4.664144515991211\n",
      "4.765714645385742\n",
      "4.70655632019043\n",
      "4.5956573486328125\n",
      "4.7101240158081055\n",
      "4.713661193847656\n",
      "4.686909198760986\n",
      "4.700076103210449\n",
      "4.718283653259277\n",
      "4.715603351593018\n",
      "4.684308052062988\n",
      "4.745601177215576\n",
      "4.735717296600342\n",
      "4.666238784790039\n",
      "4.58615255355835\n",
      "4.714625835418701\n",
      "4.671982765197754\n",
      "4.715047359466553\n",
      "4.744891166687012\n",
      "4.630162715911865\n",
      "4.707578182220459\n",
      "4.670665740966797\n",
      "4.582583427429199\n",
      "4.739546298980713\n",
      "4.674807071685791\n",
      "4.805595874786377\n",
      "4.749917507171631\n",
      "4.691989421844482\n",
      "4.604404926300049\n",
      "4.721841335296631\n",
      "4.741591930389404\n",
      "4.609963417053223\n",
      "4.662769794464111\n",
      "4.730099678039551\n",
      "4.738433361053467\n",
      "4.688235282897949\n",
      "4.639987945556641\n",
      "4.736632823944092\n",
      "4.709773540496826\n",
      "4.736939430236816\n",
      "4.69184684753418\n",
      "4.719646453857422\n",
      "4.752516746520996\n",
      "4.570086479187012\n",
      "4.643786907196045\n",
      "4.699163913726807\n",
      "4.806960105895996\n",
      "4.572142601013184\n",
      "4.717066287994385\n",
      "4.509502410888672\n",
      "4.603540897369385\n",
      "4.6649675369262695\n",
      "4.712099075317383\n",
      "4.736577033996582\n",
      "4.812878131866455\n",
      "4.596436977386475\n",
      "4.702690601348877\n",
      "4.711158752441406\n",
      "4.636075019836426\n",
      "4.706498146057129\n",
      "4.706602573394775\n",
      "4.776336193084717\n",
      "4.538954257965088\n",
      "4.595245838165283\n",
      "4.648927688598633\n",
      "4.668923854827881\n",
      "4.5940351486206055\n",
      "4.757757186889648\n",
      "4.683036804199219\n",
      "4.627960205078125\n",
      "4.809708118438721\n",
      "4.720103740692139\n",
      "4.69432258605957\n",
      "4.593808650970459\n",
      "4.627923488616943\n",
      "4.628483772277832\n",
      "4.594723224639893\n",
      "4.667132377624512\n",
      "4.563992500305176\n",
      "4.598272323608398\n",
      "4.718154430389404\n",
      "4.686451435089111\n",
      "4.5646467208862305\n",
      "4.6638665199279785\n",
      "4.6551690101623535\n",
      "4.503818511962891\n",
      "4.662378311157227\n",
      "4.597469329833984\n",
      "4.553940773010254\n",
      "4.6458845138549805\n",
      "4.658196926116943\n",
      "4.652643203735352\n",
      "4.572150230407715\n",
      "4.654421806335449\n",
      "4.505650997161865\n",
      "4.6306376457214355\n",
      "4.7071919441223145\n",
      "4.6614508628845215\n",
      "4.65630578994751\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)  # zero out gradients from previous steps\n",
    "    loss.backward()  # get the gradients for all the parameters\n",
    "    optimizer.step() # use the gradients to update tour parameters\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the more times we run the training loop, the lower the loss becomes.\n",
    "\n",
    "Let's increase the number of runs to 1000, and print at the end.\n",
    "\n",
    "Note: this is pretty janky as far as optimization goes, but we're learning so....\n",
    "\n",
    "Note 2: Keep running maybe with 10000 to continue reducing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.451270580291748\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)  # zero out gradients from previous steps\n",
    "    loss.backward()  # get the gradients for all the parameters\n",
    "    optimizer.step() # use the gradients to update tour parameters\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the post-training results (with more tokens for a longer response.)\n",
    "\n",
    "What a big difference!!  Not quite Shakespeare yet, but maybe as good as a million monkeys on keyboards?!?  :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ARICI fere yothar highe LUSeppe y chan d a t, hif nes,\n",
      "Pl peORjulaliseel E Lo sot.\n",
      "Ouprco,\n",
      "Areke atha anghars s NCHO:\n",
      "Cieneedevee, ESons, s ffaye nencawhe;\n",
      "UThis thoo lfantentROR: scor ce athecre yealldil:\n",
      "Sourorir Goban o ishead onshatie cth walot hery I h merd ply this.\n",
      "TAne bexpimond me stowan ur\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long) # creating a 1x1 tensor of zeros (remember 0 = new line)\n",
    "\n",
    "# ask for 100 new tokens, generate, convert to list to feed into decode\n",
    "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress!!  BUT Recall that this is a very simple model.  The tokens aren't talking to each other...given the previous context of what was generated, we're only looking at the previous token to make a prediction as to what comes next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the output of this notebook, and move this to the bigram.py script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mathematical trick in self-attention\n",
    "\n",
    "Pay close attention because this trick is at the heart of efficient self-attention\n",
    "\n",
    "Below, the 8 tokens are currently not talking to each other.  We want to couple them, but in a very specific way.  \n",
    "- We don't want a token talking to the tokens that come after (e.g. the 5th token talking to the 6th, 7th, or 8th) as those are future tokens that we are trying to predict.  \n",
    "- We want the tokens to talk to the previous tokens for \"conversational context\".\n",
    "\n",
    "The simplest way to \"communicate\" is to do an average of the previous tokens.  If I'm the 5th token, I take my channel plus the channels from the 4th, 3rd, etc tokens, and average them up.  This becomes a feature vector that summarizes me in the context of my history.  This is extremely lossy, but it's good enough for now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider this simple example\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 2 # B = batch size, T = time, C = channels\n",
    "x = torch.randn(B, T, C) # random input\n",
    "x.shape\n",
    "\n",
    "torch.Size([4, 8, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag of words = a term used when we're averaging things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "x_bag_of_words = torch.zeros((B, T, C))\n",
    "for b in range(B): # Not efficient but we'll improve later\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C) - basically chunk of previous tokens\n",
    "        x_bag_of_words[b, t] = torch.mean(xprev, 0) # (C,) - mean over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each element is the average of all the previous\n",
    "x_bag_of_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is inefficient.  The trick is to use matrix multiplication.\n",
    "\n",
    "Row of A * column of B = C.  \n",
    "\n",
    "Number of c repeats because A is all 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "----\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "#a = torch.ones(3,3)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "#a = a / torch.sum(a, 1, keepdim=True) # this is the same as dividing by rowsum....rows average 1\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('----')\n",
    "print('a=')\n",
    "print(a)\n",
    "print('----')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('----')\n",
    "print('c=')\n",
    "print(c)\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2 - Matrix Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2\n",
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "x_bag_of_words2 = weights @ x # (B, T, T) @ (B, T, C) -> (B, T, C) \n",
    "\n",
    "# xbow2 will be identical to xbox\n",
    "torch.allclose(x_bag_of_words, x_bag_of_words2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 3 - Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tril' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtril\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tril' is not defined"
     ]
    }
   ],
   "source": [
    "#tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros(T, T)  # Set tril initially to zero\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # Anywhere tril is 0, set to neg infinity\n",
    "weights = F.softmax(weights, dim=1) # softmax over time (normalization, exponentiate every element, divide by sum)\n",
    "x_bag_of_words3 = weights @ x # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "torch.allclose(x_bag_of_words, x_bag_of_words3)\n",
    "\n",
    "# This is like an affinity - weighted average of previous tokens.  \n",
    "# We set the future tokens to -inf so they don't contribute to the average\n",
    "# Then we normalize the weights so they sum to 1\n",
    "# (future cannot communicate with the past)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 4: Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4\n",
    "torch.manual_seed(42)\n",
    "B, T, C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B, T, C) # random input\n",
    "\n",
    "# single head for self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "weights = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "# for every row of B we get a T^2 matrix of weights\n",
    "\n",
    "# simple average of all the past tokens + current token \n",
    "# (aka lower triangular structure in weight matrix)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#weights = torch.zeros((T, T))\n",
    "# decoder block - mask out the future tokens\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # (T, T)\n",
    "weights = F.softmax(weights, dim=1) # (T, T) # exponentiate and normalize\n",
    "\n",
    "#out = weights @ x\n",
    "\n",
    "v = value(x)\n",
    "out = weights @ v\n",
    "\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0839, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0606, 0.2418, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1684, 0.0240, 0.2433, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1060, 0.2614, 0.1076, 0.3452, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1187, 0.0218, 0.0152, 0.0890, 0.0695, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0654, 0.3663, 0.0801, 0.3100, 0.6478, 0.0812, 0.0000, 0.0000],\n",
       "        [0.3644, 0.0150, 0.5266, 0.0205, 0.1464, 0.0097, 0.0605, 0.0000],\n",
       "        [0.0326, 0.0698, 0.0271, 0.2353, 0.1363, 0.9091, 0.9395, 1.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention notes:\n",
    "\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `weights` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 vectors, dot product between keys and tokens.\n",
    "\n",
    "Also note - scenarios where you might want the future tokens to \"talk to\" the past ones - sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
